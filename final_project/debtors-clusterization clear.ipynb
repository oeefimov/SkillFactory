{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Постановка задачи","metadata":{}},{"cell_type":"markdown","source":"![](https://newsland.com/static/u/comment_image_from_text/01042017/87421366-1774244.jpg)    \n     \nВ энергосбытовой компании (далее - ЭСК) есть несознательные абоненты, которые имеют задолженность за потребленную электроэнергию. С этими должниками проводится некоторая работа. А, может, не проводится, потому что часть должников вспоминает о долге и начинает платить по частям, небольшими суммами. С такими должниками обходятся довольно мягко - к ним претензии не предъявляются, от них ждут постепеннной оплаты. Мягкость обхождения с ними имеет экономические причины - работа с должниками обходится ЭСК недешево. Информирование по телефону или по электронной почте, а также взыскание через суд - эти действия при массовых объемах (несколько сотен тысяч должников) тратят значительные ресурсы компании, и не всегда траты оправдывают себя.    \n   \nЖесткого алгоритма разбиения должников на группы для разных сценариев работы по истребованию долга у ЭСК нет. В основном опираются на сумму задолженности, но она не является главным фактором, потому что, например, 5000 тыс. руб. долга для коттеджа - это долг за 1 месяц, а та же сумма для пенсионера в однокомнатной квартире - это годовая задолженность. Также нельзя опираться и на длительность долга. Некоторые абоненты могли находиться в стесненных обстоятельствах, накопить долги, но после исправления ситуации начать понемногу оплачивать. С такими должниками (а их достаточно много) для снижения социальной напряженности работы по взысканию обычно не проводятся. Их обычно просто информируют, и на этом работа заканчивается. Но есть должники, которые упорно не желают оплачивать, - с такими надо проводить весь спектр работ.    \n\nНеобходимо разбить всю массу должников на группы, которые показывали бы их отношение к задолженности и их социальное положение. Данное разбиение должно учитывать факторы, которые может предоставить биллинговая система учета электроэнергии. Результатом должны являться списки должников 3 или 4 видов: должники, не требующие внимания, требующие мягкого/умеренного внимания, требующие пристального внимания. ","metadata":{}},{"cell_type":"markdown","source":"![](https://michurinec.org/uploads/images/dolzhnik.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Библиотеки и функции","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport random\nimport scipy\nfrom scipy import spatial\n\nimport sys\nfrom datetime import datetime, timedelta\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport glob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import homogeneity_score, silhouette_score, completeness_score, v_measure_score\n\nfrom sklearn import cluster, datasets, mixture\n# from sklearn.tree import DecisionTreeClassifier, export_graphviz\n# from sklearn.neighbors import kneighbors_graph\n# from itertools import cycle, islice\nfrom scipy.spatial.distance import cosine\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install SimpSOM\nimport SimpSOM as sps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Input, Dense, LeakyReLU, Add, Activation, ZeroPadding2D, Dropout\nfrom keras.layers import BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\nfrom keras.models import Model, Sequential, load_model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\nfrom keras.initializers import glorot_uniform\nfrom keras.optimizers import SGD","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Зафиксируем воспроизводимость экспериментов\nRANDOM_SEED = 21\ntf.random.set_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функция для сравнения графиков чистых данных и их логарифмов \ndef val_log_plot(df, col):\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].hist(df[col], rwidth=0.9, alpha=0.7, bins=15)\n    ax[0].set_title(col)\n    ax[1].hist(np.log(df[col]+1), rwidth=0.9, alpha=0.7, bins=15)\n    ax[1].set_title('log of '+col)\n    plt.show()\n    \n    \n# Определение выбросов\ndef get_outlier(df, col):\n    Q3 = pd.DataFrame.quantile(df, q=0.75, axis=0, numeric_only=True, interpolation='midpoint')[col]\n    Q1 = pd.DataFrame.quantile(df, q=0.25, axis=0, numeric_only=True, interpolation='midpoint')[col]\n    IQR = round(Q3-Q1,1)\n    return df[~df[col].between(Q1 - 1.5*IQR, Q3 + 1.5*IQR)][col], Q1 - 1.5*IQR, Q3 + 1.5*IQR\n\n\n# Информация о выбросах с графиками\ndef show_info(df, col, show=True):\n    # Выводим количество выбросов и их границы\n    out, lim1, lim2 = get_outlier(df, col)\n    minCol = df[col].min()\n    maxCol = df[col].max()\n    median = df[col].median()\n    nulCol = sum(pd.isnull(df[col]))\n    \n    cnt = min(int(df[col].value_counts().count()),2000)\n    \n    if show:\n        print('Не заполнено: ', nulCol)\n        print('Минимум: ', minCol)\n        print('Максимум: ', maxCol)\n        print('Медиана: ', median)\n        print('Количество выбросов: ', len(out))\n        if len(out) > 0:\n            print('Нижняя граница выбросов: ', lim1)\n            print('Верхняя граница выбросов: ', lim2)\n\n        # Выводим графики: гистограмму и боксплот\n        fig, axes = plt.subplots(1,2,figsize=(12,4))\n        axes[0].hist(df[col], bins=cnt)\n        axes[1].boxplot(df[col])\n    \n    return {'med': median, 'lm1': lim1, 'lm2': lim2}\n\n\n# Функция генерации произвольного цвета\ndef generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: random.randint(0, 255), range(3)))\n    return color\n\n\n# Функция визуализации кластеров и расчета центроидов\ndef plot_clusters(df_clust, labels, need_pca=True, name_alorithm = ''):\n    \n    # Для визуализации кластеров многомерных объектов понизим размерность методом выделения главных компонент\n    if need_pca:\n        pca = PCA(2)\n        pca.fit(df_clust)\n        X_PCA = pca.transform(df_clust)\n        x, y = X_PCA[:, 0], X_PCA[:, 1]\n    else:\n        x, y = df_clust[:, 0], df_clust[:, 1]\n\n    # Каждому кластеру назначим свой цвет на графике\n    clust = np.unique(labels)\n    colors = {}\n    if len(clust) == 3:\n        colors[clust[0]] = 'red'\n        colors[clust[1]] = 'blue'\n        colors[clust[2]] = 'green'\n    else:\n        for i in range(len(clust)):\n            colors[clust[i]] = generate_color()\n\n    # Прорисовываем график\n    df1 = pd.DataFrame({'x': x, 'y':y, 'label':labels}) \n    groups = df1.groupby('label')\n    centroids = {}\n\n    fig, ax = plt.subplots(figsize=(10, 10)) \n\n    for name, group in groups:\n        ax.plot(group.x, group.y, marker='o', linestyle='', ms=4,\n                color=colors[name],label='cluster ' + str(name), mec='none', zorder=-1)\n        ax.set_aspect('auto')\n        ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n        ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n        \n        centroid = (sum(group.x)/len(group.x),sum(group.y)/len(group.y))\n        centroids[name] = centroid\n        ax.scatter(centroid[0], centroid[1], color='black', marker = 'X', s=200, zorder=1)\n        \n#     for i in range(len(centroids)):\n#         ax.scatter(centroids[i][0], centroids[i][1], color='black', zorder=1)\n    \n    ax.legend()\n    ax.set_title(name_alorithm + \" Кластеры должников\")\n    plt.show()\n    \n    # Проверим метрики кластеризации\n    silhouette = silhouette_score(df_clust, labels, metric='euclidean')\n    homogeneity = homogeneity_score(labels_true=y, labels_pred=labels)\n    completeness = completeness_score(labels_true=y, labels_pred=labels)\n    v_measure = v_measure_score(labels_true=y, labels_pred=labels)\n\n    print('silhouette = ', silhouette)\n    print('homogeneity = ', homogeneity)\n    print('completeness = ', completeness)\n    print('v_measure = ', v_measure)\n    \n    dfc = pd.DataFrame(centroids)\n    arr = []\n    for i in range(len(dfc.columns)):\n        arr.append(dfc[dfc.columns[i]].to_list())\n    \n    return arr\n    \ndef get_centroid(x,y):\n    return sum(x)/len(x),sum(y)/len(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Чтение данных","metadata":{}},{"cell_type":"code","source":"# Проверяем, сколько у нас файлов с сырыми данными и какого они формата\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Данные по должникам разбиты по районам Санкт-Петербурга. Районы и их должники подобраны примерно одинаковые, то есть заранее исключены далекие районы в депрессивных пригородах. Остальные районы по опыту работы в ЭСК не слишком отличаются в плане работы с задолженностью, поэтому выделять особый признак для района должника нет надобности.","metadata":{}},{"cell_type":"code","source":"# Собираем данные из папки с должниками\ndata = pd.concat([pd.read_csv(f) for f in glob.glob('../input/debtors/z*.csv')])\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Датасет собран небольшой. В реальности должников гораздо больше. Данные собирались специально так, чтобы совпадало распределение числовых величин на полном наборе и на выборочном. ","metadata":{}},{"cell_type":"code","source":"# Обзор данных\ndisplay(data.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Описание полей:**   \n    \nACODE_CODE_ORG_UNIT - идентификатор абонента ЭСК     \nBU_COUNT - количество актов нарушений учета электроэнергии    \nCOURT_COUNT - количество судебных дел по взысканию задолженности    \nDATE_CONTROL - дата последнего контрольного посещения абонента для проверки счетчика и его показаний     \nMETER_OK - признак наличия исправного счетчика       \nIS_IKUS - признак наличия у абонента личного кабинета на портале ЭСК         \nPHONE_OK - признак наличия известного и действительного (не устаревшего) номера домашнего телефона     \nMOBILE_OK - признак наличия известного и действительного (не устаревшего) номера мобильного телефона; данный признак заполняется либо по данным из личного кабинета, либо при оплате на сайте ЭСК с указанием номера мобильного телефона (номер можно указать при оплате для получения на него электронной квитанции) - примерно в равном соотношении       \nEMAIL_OK - признак наличия известного и действительного (не устаревшего) адреса эл. почты для получения счетов; данный признак заполняется либо по данным из личного кабинета, либо (очень редко) по обращению в ЭСК присылать счета на электронную почту                \nSUM_OVERDUE - сумма долга в рублях (далее все суммы даны в рублях)   \nDEB_TIME - количество месяцев задолженности     \nSHIP_SUM - общая сумма начислений у абонента    \nREAL_SUM - общая сумма оплаты у абонента     \nPAY_3 - сумма платежей за последние 3 месяца    \nPAY_6 - сумма платежей за последние 6 месяцев    \nPAY_9 - сумма платежей за последние 9 месяцев    \nPAY_12 - сумма платежей за последние 12 месяцев    \nPAY_15 - сумма платежей за последние 15 месяцев    \nPAY_18 - сумма платежей за последние 18 месяцев    \nSHP_3 - сумма начислений за последние 3 месяца    \nSHP_6 - сумма начислений за последние 6 месяцев    \nSHP_9 - сумма начислений за последние 9 месяцев    \nSHP_12 - сумма начислений за последние 12 месяцев    \nSHP_15 - сумма начислений за последние 15 месяцев    \nSHP_18 - сумма начислений за последние 18 месяцев    \n     \n**Пояснение по полям:**     \n1) Запрос по оплатам и начислениям из рабочей базы данных составлен для последних 18 месяцев. Опыт показывает, что абоненты обычно сами помнят о долгах и выстраивают план их оплаты только в пределах 1,5 лет. Все долги, более ранние, нежели этот срок, взыскать в судебном и досудебном порядке обычно крайне сложно (хотя срок давности = 3 года). Поэтому борьбу с задолженностью ведут обычно в пределах долга за последние 1,5 года.   \n    \n2) Процесс взыскания задолженности строится в зависимости от суммы и длительности долга абонента, а также от возможности информирования его по разным каналам: письмо по электронной почте, звонок по домашнему телефону, СМС на мобильный телефон, уведомление в личном кабинете, доставка письменного уведомления лично в руки. Данные о возможностях применения тех или иных каналов также включены в датасет (наличие эл. почты, мобильного телефона, регистрации в личном кабинете).  \n    \n3) В набор данных включена дополнительно информация о состоянии счетчика электроэнергии. Опыт показывает, что те абоненты, у которых счетчик исправен, не безнадежны. Те же, у кого он отсутствует или испорчен, являются злостными и принципиальными неплательщиками. Дополнительно включена информация о наличии актов нарушения учета - самовольного подключения или вмешательства в механизм счетчика для искажения показаний.    \n     \n4) Посещение контролером абонента на дому и снятие контрольных показаний часто понуждает должника начать оплачивать долг. Поэтому в датасет включена информация о дате последнего контрольного посещения.     \n      \n5) В датасете присутствует столбец с количеством судебных дел в отношении неплательщика. Обычно взыскание по суду идет в упрощенном порядке: на должника подают заявление о выдаче судебного приказа мировым судьей, сам неплательщик при этом в суд не вызывается, его просто ставят перед фактом судебного дела. Взыскание по суду обычно идет не по желанию должника - через списание из зарплаты/пенсии или с банковского счета. С теми должниками, на кого уже подано в суд, вести какие-то работы уже не имеет смысла. С них и так снимут деньги через службу судебных приставов.","metadata":{}},{"cell_type":"code","source":"# Читаем файл с шаблоном действий для каждого варианта разбиения\npattern = pd.read_csv('../input/debt-pattern/Pattern.csv')\npattern","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Описание полей:**    \n     \npir - признак кластера с самым большим количеством судебных дел     \nbig_overdue - признак кластера с самым большим долгом     \nnot_little_pay - признак кластера не с самой маленькой оплатой (то есть, средней или большой)     \ngeek - признак кластера не с самым маленьким количеством номеров мобильных телефонов и адресов электронной почты (то есть, со средним или большим)          \nneed_message - нужно ли при таких характеристиках кластера оповещение о долге (нет/по одному каналу оповещения/по всем каналам оповещения)   \nneed_pir - нужно ли при таких характеристиках кластера обращение в суд (нет/да)","metadata":{}},{"cell_type":"markdown","source":"После выделения кластеров и получения их характеристик в соответствии с данным шаблоном будут даны списки абонентов с рекомендацией по оповещению и обращению в суд.","metadata":{}},{"cell_type":"markdown","source":"# EDA & Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Преобразование строк в числа","metadata":{}},{"cell_type":"markdown","source":"Числовые показатели, отвечающие за динамику начислений и оплат, даются в строковом виде. Преобразуем их","metadata":{}},{"cell_type":"code","source":"# Числовые столбцы, переданные в строковом виде\nnum_columns = ['SUM_OVERDUE','SHIP_SUM', 'REAL_SUM', 'PAY_3', 'PAY_6',\n       'PAY_9', 'PAY_12', 'PAY_15', 'PAY_18', 'SHP_3', 'SHP_6', 'SHP_9',\n       'SHP_12', 'SHP_15', 'SHP_18']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Преобразуем строку в число\nfor col in num_columns:\n    data[col] = data[col].apply(lambda x: float(x.replace(',','.')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Смотрим на распределение числовых данных\ndisplay(data.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Признак DATE_CONTROL","metadata":{}},{"cell_type":"markdown","source":"Признак с датой последнего контрольного посещения - строковый. Преобразуем его в числовое значение.","metadata":{}},{"cell_type":"code","source":"# Смотрим, как выглядит признак с датой\ndisplay(data.DATE_CONTROL)\ndisplay(data[data.DATE_CONTROL.str.len() > 10].DATE_CONTROL)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видно, что часть дат идет с временем. Время можно отсечь, так как период до последнего посещения будем измерять в днях.","metadata":{}},{"cell_type":"code","source":"# Обрубим время в строке с датой и строку превратим в дату\ndata['DATE_CONTROL'] = data['DATE_CONTROL'].apply(lambda x: x[:10])\ndata['DATE_CONTROL'] = data['DATE_CONTROL'].apply(lambda x: datetime.strptime(x, \"%d.%m.%Y\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создадим новый признак - количество дней от даты последнего посещения контролера до даты формирования датасета\nmax_date = data['DATE_CONTROL'].max()  \ndata['control_days'] = data['DATE_CONTROL'].apply(lambda x: (max_date-x).days)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Признак SUM_OVERDUE","metadata":{}},{"cell_type":"code","source":"data.SUM_OVERDUE.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = show_info(data, 'SUM_OVERDUE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видно, что в должники попали абоненты, у которых маленькие суммы долга и даже нулевые суммы. А также есть должники, у которых суммы очень большие. Отсечем тех, у кого долг маленький (порог долга в ЭСК = 1000 руб.), и тех, у кого долг огромный (свыше 100000 руб.; должников с такой суммой в ЭСК знают поименно и работу с ними ведут отдельно, персонально с каждым).","metadata":{}},{"cell_type":"code","source":"# Проверим количество должников за пределами порогов\ndisplay(data[data.SUM_OVERDUE > 100000].shape[0])\ndisplay(data[data.SUM_OVERDUE < 1000].shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Сохраним необрезанные данные и отсечем лишних должников\ndf = data.copy()\ndf = df[df.SUM_OVERDUE.between(1000, 100000)]\ndf.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Признаки SHIP_SUM и REAL_SUM","metadata":{}},{"cell_type":"code","source":"df.REAL_SUM.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.SHIP_SUM.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видно, что распределение у данных признаков крайне неравномерное. Но для нас эти признаки совершенно не показательны. поскольку интерес представляет не абсолютная сумма, а относительная - каков процент долга или оплат от суммы всех начислений.","metadata":{}},{"cell_type":"code","source":"# Создадим новые признаки для отношения долга и оплат к полной сумме начислений\ndf['overdue'] = df['SUM_OVERDUE']/df['SHIP_SUM']\ndf['pay'] = df['REAL_SUM']/df['SHIP_SUM']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['pay'].hist(bins=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['overdue'].hist(bins=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"По графикам видно, что значительный процент должников имеют задолженность не более 10% от всех сумм начислений.    \nТакже видно, что две рассмотренные переменные тесно коррелируют между собой. Их расчет достаточно сложный, долг не равен начисления минус оплаты (в долге учитываются дополнительно иные факторы), но для моделирования один из столбцов можно будет удалить.","metadata":{}},{"cell_type":"markdown","source":"### Признаки PAY_... и SHP_...","metadata":{}},{"cell_type":"markdown","source":"Данные признаки показывают динамику оплат абонентом за последние 1,5 года.    \nИх также переведем в относительные величины.","metadata":{}},{"cell_type":"code","source":"# Получаем уровень оплат в динамике\ndf['pay3'] = df['PAY_3']/df['SHP_3']\ndf['pay6'] = df['PAY_6']/df['SHP_6']\ndf['pay9'] = df['PAY_9']/df['SHP_9']\ndf['pay12'] = df['PAY_12']/df['SHP_12']\ndf['pay15'] = df['PAY_15']/df['SHP_15']\ndf['pay18'] = df['PAY_18']/df['SHP_18']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим, сколько незаполненных данных\nprint('pay3 is null: ' , len(df[df['pay3'].isnull()]))\nprint('pay6 is null: ' , len(df[df['pay6'].isnull()]))\nprint('pay9 is null: ' , len(df[df['pay9'].isnull()]))\nprint('pay12 is null: ' , len(df[df['pay12'].isnull()]))\nprint('pay15 is null: ' , len(df[df['pay15'].isnull()]))\nprint('pay18 is null: ' , len(df[df['pay18'].isnull()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Все числовые столбцы с начислениями и оплатами в изначальном датасете были заполнены. Пустые значения в сгенерированных столбцах могли появиться из-за того, что было деление ноль на ноль, то есть, сумма начислений была нулевой. Проверим это.","metadata":{}},{"cell_type":"code","source":"# Выбираем все значения SHP_... при pay...=null\nprint(df[df['pay3'].isnull()].SHP_3.unique())\nprint(df[df['pay6'].isnull()].SHP_6.unique())\nprint(df[df['pay9'].isnull()].SHP_9.unique())\nprint(df[df['pay12'].isnull()].SHP_12.unique())\nprint(df[df['pay15'].isnull()].SHP_15.unique())\nprint(df[df['pay18'].isnull()].SHP_18.unique())\n\n# Пример получения значения NaN\nser = df[df['pay3'].isnull()].iloc[0]\nprint(ser.PAY_3, '/', ser.SHP_3, '= ', ser.pay3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Если начислений нет, считаем, что все они оплачены.","metadata":{}},{"cell_type":"code","source":"# Заполним пропуски 1, то есть коэффициентом полной оплаты\ndf['pay3'] = df['pay3'].fillna(1)\ndf['pay6'] = df['pay6'].fillna(1)\ndf['pay9'] = df['pay9'].fillna(1)\ndf['pay12'] = df['pay12'].fillna(1)\ndf['pay15'] = df['pay15'].fillna(1)\ndf['pay18'] = df['pay18'].fillna(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим, сколько данных бесконечным значением\nprint('pay3 is infimum: ' , len(df[~np.isfinite(df['pay3'])]))\nprint('pay6 is infimum: ' , len(df[~np.isfinite(df['pay6'])]))\nprint('pay9 is infimum: ' , len(df[~np.isfinite(df['pay9'])]))\nprint('pay12 is infimum: ' , len(df[~np.isfinite(df['pay12'])]))\nprint('pay15 is infimum: ' , len(df[~np.isfinite(df['pay15'])]))\nprint('pay18 is infimum: ' , len(df[~np.isfinite(df['pay18'])]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Бесконечные значения в сгенерированных столбцах могли появиться из-за того, что было деление ненулевого значения на ноль, то есть, сумма начислений была нулевой, а платежи > 0. То есть, платежи покрывали долги совсем старых периодов. Проверим это.","metadata":{}},{"cell_type":"code","source":"print(df[~np.isfinite(df['pay3'])].SHP_3.unique())\nprint(df[~np.isfinite(df['pay6'])].SHP_6.unique())\nprint(df[~np.isfinite(df['pay9'])].SHP_9.unique())\nprint(df[~np.isfinite(df['pay12'])].SHP_12.unique())\nprint(df[~np.isfinite(df['pay15'])].SHP_15.unique())\nprint(df[~np.isfinite(df['pay18'])].SHP_18.unique())\n\n# Пример получения значения Inf\nser = df[~np.isfinite(df['pay3'])].iloc[0]\nser.PAY_3\nprint(ser.PAY_3, '/', ser.SHP_3, '= ', ser.pay3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Бесконечные значения заменим единицей, то есть коэффициентом полной оплаты\ndf['pay3'] = df['pay3'].apply(lambda x: 1 if not np.isfinite(x) else x)\ndf['pay6'] = df['pay6'].apply(lambda x: 1 if not np.isfinite(x) else x)\ndf['pay9'] = df['pay9'].apply(lambda x: 1 if not np.isfinite(x) else x)\ndf['pay12'] = df['pay12'].apply(lambda x: 1 if not np.isfinite(x) else x)\ndf['pay15'] = df['pay15'].apply(lambda x: 1 if not np.isfinite(x) else x)\ndf['pay18'] = df['pay18'].apply(lambda x: 1 if not np.isfinite(x) else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим, нет ли случайно отрицательных платежей\ndf.pay3.min(), df.pay6.min(), df.pay9.min(), df.pay12.min(), df.pay15.min(), df.pay18.min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим количество отрицательных платежей\nlen(df[df.pay3<0]), len(df[df.pay6<0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Удалим явно ошибочные строки с отрицательными платежами\ndf = df[(df.pay3>=0) & (df.pay6>=0)]\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Признак DEB_TIME","metadata":{}},{"cell_type":"markdown","source":"У данного признака были пропуски. Посмотрим, сколько их и какие это данные.","metadata":{}},{"cell_type":"code","source":"df[df.DEB_TIME.isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"После отсечения малых долгов в этом столбце все данные оказались заполнены. ","metadata":{}},{"cell_type":"markdown","source":"### Признаки BU_COUNT и COURT_COUNT","metadata":{}},{"cell_type":"code","source":"# Посмотрим на распределение данных признаков\nbin_columns = ['BU_COUNT', 'COURT_COUNT']\ni = 0\nfig, ax = plt.subplots(1, 2, figsize=(8, 3))\nfor col in bin_columns:\n    ax[i].hist(df[col], rwidth=0.9, alpha=0.7, bins=15)\n    ax[i].set_title(col)\n    i += 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видно из графика, признак BU_COUNT оказался неинформативным, так как все его значения = 0. Скорее всего, это ошибка в скрипте, который использовался для набора данных. Исключим этот признак из моделирования.","metadata":{}},{"cell_type":"markdown","source":"### Распределение числовых признаков","metadata":{}},{"cell_type":"markdown","source":"Как видно было из сводного описания датасета, многие числовые признаки имели большой разброс в данных.","metadata":{}},{"cell_type":"code","source":"# Проверим на распределение числовых данных, которые будут использоваться для кластеризации\nfor col in ['COURT_COUNT', 'DEB_TIME', 'control_days', 'overdue', 'pay3',\n       'pay6', 'pay9', 'pay12', 'pay15', 'pay18']:\n    val_log_plot(df, col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Распределение после логарифмирования улучшилось либо осталось неизменным для всех числовых признаков. Есть смысл заменить все числовые признаки их логарифмом.","metadata":{}},{"cell_type":"code","source":"# Логарифмируем числовые признаки\nfor col in ['COURT_COUNT', 'DEB_TIME', 'control_days', 'overdue', 'pay3',\n       'pay6', 'pay9', 'pay12', 'pay15', 'pay18']:\n        df[col+'_log'] = np.log(df[col]+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Бинарные признаки","metadata":{}},{"cell_type":"code","source":"# Посмотрим на распределение бинарных признаков\nbin_columns = ['METER_OK', 'IS_IKUS', 'PHONE_OK', 'MOBILE_OK', 'EMAIL_OK']\ni = 0\nfig, ax = plt.subplots(1, 5, figsize=(18, 2))\nfor col in bin_columns:\n    ax[i].hist(df[col], rwidth=0.9, alpha=0.7, bins=15)\n    ax[i].set_title(col)\n    i += 1\nplt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"По графику видно, что признак METER_OK неинформативен, так как все его значения = 1. Исключим его из модели","metadata":{}},{"cell_type":"markdown","source":"#### Данные для кластеризации","metadata":{}},{"cell_type":"code","source":"# Проверим столбцы, которые в итоге есть у датасета\ndf.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Выделим столбцы для применения кластеризации\ndf = df[['IS_IKUS', 'PHONE_OK', 'MOBILE_OK', 'EMAIL_OK', \n       'COURT_COUNT_log', 'DEB_TIME_log', 'control_days_log', 'overdue_log', \n       'pay3_log', 'pay6_log', 'pay9_log', 'pay12_log', 'pay15_log', 'pay18_log']] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим корреляцию столбцов в сборном датасете\ncorrelation = df.corr()\nplt.figure(figsize=(16, 12))\nsns.heatmap(correlation, annot=True, cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Тепловая карта корреляции показала, что наличие действующего адреса электронной почты и регистрации в личном кабинете сильно коррелируют друг с другом, что вполне объяснимо: именно из личного кабинета в подавляющем большинстве случаев приходит информация об электронном адресе. Столбец IS_IKUS исключим из модели.","metadata":{}},{"cell_type":"markdown","source":"Опытным путем было выяснено, что признак control_days_log (количество дней от последнего контрлольного посещения) на разбиение на кластеры совсем не влияет - во всех кластерах его распределение абсолютно одинаковое.Кроме того, при наличии этого признака области кластеров начинали пересекаться. В результате признак был удален из моделирования.      \nТакже опытным путем было выяснено, что наличие домашнего (стационарного) телефона тоже не оказывает влияния на кластеры. Домашние телефоны есть практически у всех, их номера получены компанией не за счет передачи их лично абонентом, а по телефонным справочникам советских годов. Признак был удален из моделирования.","metadata":{}},{"cell_type":"code","source":"df = df[[ 'MOBILE_OK', 'EMAIL_OK', # 'IS_IKUS', 'control_days_log', 'PHONE_OK',\n       'COURT_COUNT_log', 'DEB_TIME_log',  'overdue_log', \n       'pay3_log', 'pay6_log', 'pay9_log', 'pay12_log', 'pay15_log', 'pay18_log']] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Теплый\" угол со столбцами уровня оплаты очень сильно коррелируют, особенно ближайшие соседи по периодам, но удалять их не будем, так как они демонстрируют динамику оплаты.","metadata":{}},{"cell_type":"code","source":"# Нормализуем данные\nscaler = StandardScaler()\ndf_sc = scaler.fit_transform(df)\ndf_sc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Датасет для кластеризации готов.","metadata":{}},{"cell_type":"code","source":"# Запомним количество признаков\ninput_num = df_sc.shape[1] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Кластеризация","metadata":{}},{"cell_type":"markdown","source":"### Чистый ML: K-Means","metadata":{}},{"cell_type":"markdown","source":"Сначала применим классический алгоритм Machine Learning для кластеризации - это K-Means.      \nВыбор этого метода кластеризации обусловлен:   \n1) малым количеством кластеров,     \n2) средним количеством признаков,    \n2) необособленностью кластеров и их предполагаемой выпуклостью (скорее всего придется просто делить равномерное распределение объектов в пространстве признаков).","metadata":{}},{"cell_type":"code","source":"# Кластеризуем\nkmeans = KMeans(n_clusters=3,max_iter=300,random_state=RANDOM_SEED)\nkmeans.fit(df_sc)   \nlabels1 = kmeans.labels_.astype(np.int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Выведем на график кластеры и метрики кластеризации, предварительно понизив размерность методом выделения главных компонент \ncentroids1 = plot_clusters(df_sc, labels1, name_alorithm = 'KMeans')\nprint('centroids: ')\nprint(centroids1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Кластеры тесно смыкаются друг с другом, что означает, что пространство признаков должников довольно размыто и однородно.    \n","metadata":{}},{"cell_type":"markdown","source":"Метрики показывают не слишком хороший результат, но это обусловлено нечеткостью и размытостью состава должников.","metadata":{}},{"cell_type":"code","source":"# Проверим распределение величин по каждому признаку для разных классов\ndf['cl_kmeans'] = labels1\n\nfor c in df:\n    grid= sns.FacetGrid(df, col='cl_kmeans')\n    grid.map(plt.hist, c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Для быстрой интерпретации результата выведем сравнение средних значений по каждому кластеру/величин\nfor col in df.columns[:input_num]:\n    fig, ax = plt.subplots(1, 2, figsize=(7,3))\n    ax[0].set_title('Mean ' + col)\n    df.groupby(by = 'cl_kmeans').mean()[col].plot(ax=ax[0], kind='bar')\n    ax[1].set_title('Median ' + col)\n    df.groupby(by = 'cl_kmeans').median()[col].plot(ax=ax[1], kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим количественное распределение\ndf['cl_kmeans'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Судя по распределению, кластеры были выделены так:    \n**Кластер 0**:  небольшой долг, платят не слишком активно, технически продвинутые (есть e-mail и/или регистрация в личном кабинете, используют мобильное приложение и/или при оплате указывают мобильный телефон), судебных дел практически нет. В кластер 1 попало подавляющее большинство должников.       \n**Кластер 1**: долг среднего размера или небольшой, платят очень активно, технически продвинуты, есть некоторое количество судебных дел.    \n**Кластер 2**: большой долг, платят плохо, технически не продвинутые, много судебных дел. Это самый немногочисленный кластер, в него попали самые злостные маргинализированные абоненты.","metadata":{}},{"cell_type":"markdown","source":"Рекомендации для кластеров должников таковы:     \n1) Для кластера 0 желательно оповестить о задолженности по всем доступным каналам (СМС, e-mail, личный кабинет). Так как абоненты из этого кластера пользуются гаджетами, уведомления с большой вероятностью будут ими прочитаны, а небольшой долг оплатить менее сложно, чем большой. Судебная работа не нужна.    \n2) Для кластера 1 достаточно оповестить о задолженности по какому-либо одному каналу, а при недостатке средств возможно даже не оповещать, так как данная группа активно платит и тем самым сокращает долг. Уведомления будут прочитаны, так как кластер технически продвинут.    \n3) Для кластера 2 лучше всего не тратить средства на дорогие оповещения, так как маловероятно, что они будут прочитаны, а лучше сразу выходить в суд, если до сих пор в суд не выходили. Если судебные дела уже ведутся, оповещать не нужно.","metadata":{}},{"cell_type":"markdown","source":"### Сеть Кохонена","metadata":{}},{"cell_type":"markdown","source":"Идея кластеризации с использованием нейронной сети Кохонена (она же сеть SOM - Self Organization Maps) заключается в том, чтобы преобразовать многомерное пространство признаков в более простое низкоразмерное пространство, сохранив при этом внутреннюю топологию данных. Фактические расстояния будут потеряны после преобразования SOM, но внутренняя структура данных сохранится.     \nСети Кохонена полезны, когда данных и их признаков очень много. За счет понижения размерности значительно сокращаются вычислительные ресурсы.","metadata":{}},{"cell_type":"markdown","source":"![Сеть Кохонена](https://ranalytics.github.io/data-mining/figures/cohonen_activation.png) ","metadata":{}},{"cell_type":"markdown","source":"Алгоритм кластеризации с помощью сети Кохонена таков:    \n1) Подготовленный набор данных сначала прогоним через сеть SOM, получив на выходе карту с небольшими количеством ячеек.   \n2) Карту SOM отправим на кластеризацию методом K-Means.","metadata":{}},{"cell_type":"code","source":"# Создадим сеть Кохонена размером 20 на 20 выходных нейронов и активируем периодические граничные условия (PBC)\nsomModel = sps.somNet(20, 20, df_sc) #, PBC=True\n\n# Обучим сеть в течение 1000 эпох с шагом learning rate = 0.01\nsomModel.train(0.01, 1000) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Получим карту признаков сниженной размерности, спроектировав наш датасет на плоскость при помощи обученной сети \nmap_ = np.array((somModel.project(df_sc)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим размерность карты\nmap_.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Визуализируем полученную карту. На графиках весов и расстояний между ячейками карты уже будут заметны области с разными характеристиками. Их алгоритм K-Means, скорее всего, и выделит в отдельные кластеры. Проверим это.","metadata":{}},{"cell_type":"code","source":"# Визуализируем веса каждой ячейки карты\nsomModel.nodes_graph(colnum=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Визуализируем расстояния каждой ячейки карты до ее соседей\nsomModel.diff_graph(show=True,printout=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Кластеризуем преобразованные данные\nkmeans = KMeans(n_clusters=3,max_iter=300,random_state=RANDOM_SEED)\nkmeans.fit(map_)   \nlabels2 = kmeans.labels_.astype(np.int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Визуализируем кластеры\ncentroids2 = plot_clusters(map_, labels2, need_pca=False)\nprint('centroids:')\nprint(centroids2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В принципе, данный график повторяет области на картах с весами и расстояниями.","metadata":{}},{"cell_type":"code","source":"# Выведем информацию из разных кластеров, чтобы можно было интерпретировать результат\ndf['cl_som'] = labels2\n\nfor col in df.columns[:input_num]:\n    fig, ax = plt.subplots(1, 2, figsize=(7,3))\n    ax[0].set_title('Mean ' + col)\n    df.groupby(by = 'cl_som').mean()[col].plot(ax=ax[0], kind='bar')\n    ax[1].set_title('Median ' + col)\n    df.groupby(by = 'cl_som').median()[col].plot(ax=ax[1], kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Проверим количественное распределение кластеров\ndf.cl_som.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"По данным графикам видно, что сеть Кохонена разделила должников по иному принципу - по принципу их оплаты. Кластеры четко соответствуют динамике оплаты - от самой низкой оплаты до самой высокой. К сожалению, в API библиотеки simpSOM нет возможности зафиксировать начальную инициализацию весов (она всегда происходит случайно) и тем самым установить воспроизводимость результата. Поэтому разбиение на кластеры может получаться иным с каждым новым прогоном через сеть. Но в нашем случае сеть Кохонена стабильно делит на кластеры  по принципу их оплаты. При этом разбиение по сумме долга и наличию судебных дел может меняться.    \n    \nЧаще всего получается кластеризация, совпадающая с результатами K-Means:      \n**Кластер 0:** должники с самым большим долгом и самыми маленькими оплатами, технически не продвинутые, судебных дел много. То есть, это злостные маргинализированные неплательщики.         \n**Кластер 1:** должники с самым маленьким уровнем долга, с не слишком активными оплатами. Технически продвинуты, судебных дел мало.     \n**Кластер 2:** должники со средним уровнем долга, с самыми активными оплатами, судебных дел среднее количество, технически  продвинутые.  \nРекомендации по ведению работы с ними тоже совпадают с рекомендациями для разбиения методом K-Means:     \nДля кластера 0 - оповещение не имеет смысла, необходимо судиться, если до сих пор не вышли в суд, и ничего не делать, если в суд уже вышли.\nДля кластера 1 - оповещать по всем каналам. В суд идти не надо.\nДля кластера 2 - оповестить по одному каналу, а при недостатке средств вовсе не оповещать, так как данный кластер неплохо оплачивает долги. В суд идти не надо, достаточно досудить старые дела.\n     \nНо также встречается второй вариант кластеризации:    \n**Кластер 0:** должники с самым большим долгом и самыми большими оплатами, технически не продвинутые, судебных дел много. То есть, это должники, с которых принудительно через приставов уже взыскиваются долги. Рекомендация - ничего не делать с ними, поскольку суды уже идут и долги постепеннно погашаются.               \n**Кластер 1:** должники со средним уровнем долга и с малым уровнем оплат. Технически продвинуты средне, судебных дел мало. Рекомендации - оповещать по всем каналам, если есть средства оповещения, и выходить в суд, если средств оповещения нет.          \n**Кластер 2:** должники с малым долгом и средним уровнем оплаты, судебных дел практически нет, технически  продвинутые. Рекомендации - оповестить по одному каналу.","metadata":{}},{"cell_type":"markdown","source":"Но в целом разбиение каждый раз новое и интерпретируется тоже каждый раз по-новому.","metadata":{}},{"cell_type":"code","source":"# Проверим распределение величин по каждому признаку для разных классов\nfor c in df:\n    grid= sns.FacetGrid(df, col='cl_som')\n    grid.map(plt.hist, c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Autoencoder","metadata":{"execution":{"iopub.status.busy":"2021-08-22T12:12:27.470412Z","iopub.execute_input":"2021-08-22T12:12:27.47104Z","iopub.status.idle":"2021-08-22T12:12:27.616813Z","shell.execute_reply.started":"2021-08-22T12:12:27.470997Z","shell.execute_reply":"2021-08-22T12:12:27.613465Z"}}},{"cell_type":"markdown","source":"Идея кластеризации с использованием нейронной сети типа Autoencoder тоже, как и в случае сети Кохонена, заключается в том, чтобы преобразовать многомерное пространство признаков в более простое низкоразмерное пространство, сохранив при этом глубинные характеристики данных. Для этого строится нейронная сеть с двумя частями: декодер, который сворачивает размерность до заданного уровня и энкодер, который восстанавливает размерность. \"Бутылочное горлышко\", которое получается на слое с минимальной размерностью (скрытый слой, latent dense), подается в классический алгоритм кластеризации.        \nСеть Autoencoder полезна, когда признаков очень много. За счет понижения размерности значительно сокращаются вычислительные ресурсы. Кроме того, уровень скрытого слоя хранит самую существенную информацию об объекте и кластеризация на его основе проводится более чисто.","metadata":{}},{"cell_type":"markdown","source":"![](https://www.researchgate.net/profile/Kamran-Kowsari/publication/332330221/figure/fig2/AS:746162701217795@1554910459741/Structure-of-clustering-model-with-autoencoder-and-K-means-combination.png)","metadata":{}},{"cell_type":"code","source":"df_sc.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Зададим параметры для автоэнкодера\nencoding_dim = 6           # количество нейронов в \"бутылочном горлышке\"\ninput_num = df_sc.shape[1] # количество признаков","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Построим сеть для автоэнкодера\n\n# Кодирование\ninput_df = Input(shape=(input_num,)) \nx = Dense(encoding_dim, activation='relu')(input_df)\nx = Dense(500, activation='relu')(x)\nx = Dense(1000, activation='relu')(x)\nx = Dense(2000, activation='relu')(x)\nencoded = Dense(encoding_dim, activation='relu')(x)\nencoder = Model(input_df, encoded)\n\n# Декодирование\nx = Dense(2000, activation='relu')(encoded)\nx = Dense(1000, activation='relu')(encoded)\nx = Dense(500, activation='relu')(x) # попробовать другую функцию активации\ndecoded = Dense(input_num)(x)\nautoencoder = Model(input_df, decoded)\n\n# Компилируем автоэнкодер\nautoencoder.compile(optimizer= 'adam', loss='mean_squared_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Описываем callback\n# Так как нам для кластеризации, в отличие от предсказания не требуется идеальная точность,\n#    управлять шагом обучения (снижать на плато/задавать шедулер) не станем. \ncheckpoint = ModelCheckpoint('../working/best_model.hdf5' , monitor=['loss'], verbose=0  , mode='min')\nearlystop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True,)\n# callbacks_list = [checkpoint, earlystop]\ncallbacks_list = [earlystop]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Обучаем сеть\nautoencoder.fit(df_sc, df_sc, \n                batch_size = 128, \n                epochs = 100,  \n                callbacks=callbacks_list,\n                verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Кодируем наш набор данных для снижения размерности и получения скрытого слоя\npred = encoder.predict(df_sc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функция для сбора методов кластеризации\n# Кластеризуем только теми методами, у которых можно задать количество кластеров\ndef generate_clustering_algorithms(Z,n_clusters):\n    kmeans = cluster.KMeans(n_clusters=n_clusters, random_state = RANDOM_SEED)\n    agglomerative = cluster.AgglomerativeClustering(n_clusters=n_clusters)\n    birch = cluster.Birch(n_clusters=n_clusters)\n    gmm = mixture.GaussianMixture(n_components=n_clusters)\n\n    clustering_algorithms = (\n        ('KMeans', kmeans),\n        ('AgglomerativeClustering', agglomerative),\n        ('GaussianMixture', gmm),\n        ('Birch', birch)\n    )\n    return clustering_algorithms\n\n# Кластеризуем данные скрытого слоя и визуализируем разбиение\ncentroids = []\nclustering_algorithms = generate_clustering_algorithms(pred,3)\n\nplt.figure(figsize=(10 * 2 + 2, 15))\nplt.subplots_adjust(left=.02, right=.98, bottom=.01, top=.98, wspace=.05,\n                    hspace=.01)\n\nfor name, algorithm in clustering_algorithms:\n    algorithm.fit(pred)\n\n    if hasattr(algorithm, 'labels_'):\n        lbls = algorithm.labels_.astype(np.int)\n    else:\n        lbls = algorithm.predict(pred)       \n        \n    df['cl_ae_'+name[:2]] = lbls\n\n    centroid = plot_clusters(pred, lbls, name_alorithm = name)\n    centroids.append(centroid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Все разбиения схожи, кроме разбиения методом BIRCH.","metadata":{}},{"cell_type":"code","source":"# Пространство объектов имеет структуру с неявно выраженными кластерами. \n# Проверим, как разобьют на кластеры методы, в которых их количество определяется автоматически\ndbs = DBSCAN(eps=0.7, min_samples=90)\nlbls = dbs.fit_predict(pred)\ncDBSC = plot_clusters(pred, lbls)\n\nop = cluster.OPTICS(eps=0.8, min_samples=10)\nop.fit_predict(pred)\nlbls = op.labels_\ncOPT = plot_clusters(pred, lbls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Оба этих метода дают крайне неподходящий вариант разбиения, что свидетельствует о рыхлой структуре множества без явно выделенных сегментов.","metadata":{}},{"cell_type":"code","source":"# Находим попарные расстояния между центроидом разбиения 0 и центроидами остальных разбиений (кроме BIRCH)\nd_mat1 = spatial.distance.cdist(np.array(centroids[0]), np.array(centroids[1]))\nd_mat2 = spatial.distance.cdist(np.array(centroids[0]), np.array(centroids[2]))\n\n# Выбираем наиболее маленькие расстояния в каждой строке\n# Индекс самого маленького расстояния - это номер кластера в разбиении i, \n#    который соответствует кластеру в разбиении 0.\n# Иными словами, мы красим (именуем) кластеры в разных разбиениях так же, как они покрашены (поименованы) в разбиении 0\ndict1 = {}\nfor i in range(d_mat1.shape[0]):\n    dict1[i] = np.argmin(d_mat1[i])\ndict2 = {}\nfor i in range(d_mat2.shape[0]):\n    dict2[i] = np.argmin(d_mat2[i])\n\n# Перекрашиваем кластеры\ndf['cl_ae_Ag'] = df['cl_ae_Ag'].map(dict1)\ndf['cl_ae_Ga'] = df['cl_ae_Ga'].map(dict2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теоретически может оказаться, что 2 минимальных значения из разных строк оказались в одном столбце, но тогда можно в качестве образца по очереди взять другие деления на кластеры и сравнивать с ними, добиваясь, чтобы столбцы не повторялись.","metadata":{}},{"cell_type":"code","source":"# Проверим количественные распределения кластеров в разных разбиениях\ndisplay(df['cl_ae_KM'].value_counts())\ndisplay(df['cl_ae_Ga'].value_counts())\ndisplay(df['cl_ae_Ag'].value_counts())\ndisplay(df['cl_ae_Bi'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим распределения отдельных признаков для разного вида кластеров в каждом разбиении","metadata":{"execution":{"iopub.status.busy":"2021-08-22T21:34:47.325764Z","iopub.execute_input":"2021-08-22T21:34:47.3262Z","iopub.status.idle":"2021-08-22T21:34:47.332286Z","shell.execute_reply.started":"2021-08-22T21:34:47.326165Z","shell.execute_reply":"2021-08-22T21:34:47.331213Z"}}},{"cell_type":"code","source":"# K-Means\nfor col in df.columns[:7]:\n    fig, ax = plt.subplots(1, 2, figsize=(7,3))\n    ax[0].set_title('Mean ' + col)\n    df.groupby(by = 'cl_ae_KM').mean()[col].plot(ax=ax[0], kind='bar')\n    ax[1].set_title('Median ' + col) \n    df.groupby(by = 'cl_ae_KM').median()[col].plot(ax=ax[1], kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggl\nfor col in df.columns[:7]:\n    fig, ax = plt.subplots(1, 2, figsize=(7,3))\n    ax[0].set_title('Mean ' + col)\n    df.groupby(by = 'cl_ae_Ag').mean()[col].plot(ax=ax[0], kind='bar')\n    ax[1].set_title('Median ' + col)\n    df.groupby(by = 'cl_ae_Ag').median()[col].plot(ax=ax[1], kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GM\nfor col in df.columns[:7]:\n    fig, ax = plt.subplots(1, 2, figsize=(7,3))\n    ax[0].set_title('Mean ' + col)\n    df.groupby(by = 'cl_ae_Ga').mean()[col].plot(ax=ax[0], kind='bar')\n    ax[1].set_title('Median ' + col)\n    df.groupby(by = 'cl_ae_Ga').median()[col].plot(ax=ax[1], kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Делаем ансамбль по голосованию\nclusters = []\nclusters.append(df['cl_ae_KM'].values)\nclusters.append(df['cl_ae_Ga'].values)\nclusters.append(df['cl_ae_Ag'].values)\n\nclusters = scipy.stats.mode(clusters)[0]\nclusters[0]\ndf['cl_ae_mode'] = clusters[0]\ndf['cl_ae_mode'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Распределение в ансамбле\nfor col in df.columns[:7]:\n    fig, ax = plt.subplots(1, 2, figsize=(7,3))\n    ax[0].set_title('Mean ' + col)\n    df.groupby(by = 'cl_ae_mode').mean()[col].plot(ax=ax[0], kind='bar')\n    ax[1].set_title('Median ' + col)\n    df.groupby(by = 'cl_ae_mode').median()[col].plot(ax=ax[1], kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# визуализируем ансамбль\ncentroids_ens = plot_clusters(pred, df['cl_ae_mode'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Выдача результатов по сегментации должников","metadata":{}},{"cell_type":"markdown","source":"Результатом анализа должны стать списки абонентов с рекомендацией для каждого по необходимости оповещения о задолженности и/или необходимости подачи на них в суд. Для этого для каждого кластера будут рассчитаны простейшие статистики и на основании этих статистик (средних величин) будут выданы рекомендации. Рекомендации будут даваться в соответствии с приложенным шаблоном действий. Шаблон заполнялся заказчиком и описывает, что надо делать с должником в зависимости от средних статистик его кластера.","metadata":{}},{"cell_type":"markdown","source":"В результате будут выданы 4 возможные рекомендации:     \n1) Для кластеров, полученных простым применением KMeans (колонка с этим разбиением = cl_kmeans),      \n2) Для кластеров, полученных понижением размерности при помощи SOM-сети Кохонена (колонка с этим разбиением = cl_som),     \n3) Для кластеров, полученных понижением размерности при помощи автоэнкодера и ансамбля кластерных методов (колонка с этим разбиением = cl_ae_mode),     \n4) Для кластеров, полученных понижением размерности при помощи автоэнкодера и кластерного метода BIRCH (колонка с этим разбиением = cl_ae_Bi).","metadata":{}},{"cell_type":"markdown","source":"Из этих четырех разбиений должников на сегменты пользователь самостоятельно выберет тот, что кажется ему более подходящим.","metadata":{}},{"cell_type":"markdown","source":"### Подсчет статистики по признакам и применение шаблона действий","metadata":{}},{"cell_type":"code","source":"# Вычисляем статистики, то есть условия применения действия в шаблоне для разных разбиений\ndf_res = df.copy()\nfor col in ['cl_kmeans', 'cl_som', 'cl_ae_mode', 'cl_ae_Bi']:\n    # Оснащенность кластера мобильными телефонами и электронной почтой\n    a = df_res.groupby(by = col).EMAIL_OK.mean()\n    b = df_res.groupby(by = col).MOBILE_OK.mean()\n    geek = np.argmin(a+b)\n    df_res['geek'] = df_res[col].apply(lambda x: 0 if x==geek else 1)\n    \n    # Наличие судебных дел\n    pir = np.argmax(df_res.groupby(by = col).COURT_COUNT_log.mean())\n    df_res['pir'] = df_res[col].apply(lambda x: 1 if x==pir else 0)\n    \n    # Наличие большого долга\n    big_overdue = np.argmax(df_res.groupby(by = col).overdue_log.mean())\n    df_res['big_overdue'] = df_res[col].apply(lambda x: 1 if x==big_overdue else 0)\n    \n    # Отсутствие оплат\n    pay3 = np.argmin(df_res.groupby(by = col).pay3_log.mean())\n    pay6 = np.argmin(df_res.groupby(by = col).pay6_log.mean())\n    pay9 = np.argmin(df_res.groupby(by = col).pay9_log.mean())\n    pay12 = np.argmin(df_res.groupby(by = col).pay12_log.mean())\n    pay15 = np.argmin(df_res.groupby(by = col).pay15_log.mean())\n    pay18 = np.argmin(df_res.groupby(by = col).pay18_log.mean())\n\n    payments = scipy.stats.mode([pay3,pay6,pay9,pay12,pay15,pay18])[0]\n    p = payments[0]\n    df_res['not_little_pay'] = df_res[col].apply(lambda x: 0 if x==p else 1)\n    \n    # Применение шаблона\n    df_res = df_res.merge(pattern, on=['pir', 'big_overdue', 'not_little_pay', 'geek'], how='inner')\n    \n    # Переименование столбцов для следующей итерации цикла\n    df_res = df_res.rename(columns = {'geek': 'geek_'+col, \n                                      'pir': 'pir_'+col,\n                                      'big_overdue': 'big_overdue_'+col,\n                                      'not_little_pay': 'not_little_pay_'+col,\n                                      'need_message': 'need_message_'+col,\n                                      'need_pir': 'need_pir_'+col\n                                     })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Применяем шаблон по вычисленной статистике и выводим на графике распределение\nfor col in ['cl_kmeans', 'cl_som', 'cl_ae_mode', 'cl_ae_Bi']:\n    tt = pd.DataFrame(columns=['Метод','Количество'])\n    for nm in pattern.need_message.unique():\n        for np in pattern.need_pir.unique():\n            temp = df_res[(df_res['need_message_'+col]==nm) & (df_res['need_pir_'+col]==np)]\n            tt.loc[len(tt),'Метод'] = 'Оповещение = ' + nm + '. Судебная работа = ' + np\n            tt.loc[len(tt)-1,'Количество'] = len(temp)\n    ax = tt.groupby(by = ['Метод']).sum().plot.barh()\n    ax.set_title('Результаты для ' + col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Сохранение результата","metadata":{}},{"cell_type":"code","source":"df_res.to_csv('DebtorSegmentation.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Заключение","metadata":{}},{"cell_type":"markdown","source":"**Решение поставленной задачи было разбито на следующие этапы:**     \n1. Создание SQL-запроса к рабочей базе данных и выгрузка в файл информации по должникам;     \n2. Чтение, анализ и преобразование данных из файла информации по должникам;    \n3. Подготовка данных для кластеризации;       \n4. Кластеризация классическим ML-методом K-Means;     \n5. Понижение размерности пространства признаков с применением DL-методов;     \n6. Кластеризация пространства пониженной размерности набором ML-методов и ансамблирование этих методов;     \n7. Определение метрик кластеризации: обособленность (силуэт), однородность, полнота и v-мера (однородность + полнота);    \n7. Подсчет статистик по кластерам и применение шаблона работы с должниками на основании этих статистик.  ","metadata":{}},{"cell_type":"markdown","source":"**В ходе подготовки данных для кластеризации было выполнено:**     \n* Преобразование строковых данных в числовые форматы;     \n* Визуализация данных и проверка распределения;    \n* Логарифмирование числовых данных для приближения распределения к нормальному;    \n* Частичное удаление выбросов по критически важному признаку;    \n* Генерация новых признаков;     \n* Проверка корреляции и удаление из модели корелирующих признаков;    \n* Нормализация данных при помощи StandardScaler.","metadata":{}},{"cell_type":"markdown","source":"**Кластеризация методом K-Means показала:**     \n* Пространство признаков достаточно рыхлое и распределено без явных обособленных кластеров. Метрики это подтверждают.      \n* Метрики кластеризации:      \n  - silhouette =  0.28     \n  - homogeneity =  0.18      \n  - completeness = 1.00    \n* Превалирующими факторами в сегментировании стали величина долга и стремление его оплатить. Явно была выделена группа абонентов, имеющих большой долг и не желающих его оплачивать.    \n* Факторы наличия контактных данных на сегментирование повлияло мало. ","metadata":{}},{"cell_type":"markdown","source":"**Шаги кластеризации с понижением размерности при помощи SOM-сети:**   \n* Применение сети Кохонена с использованием библиотеки simpSOM;     \n* Визуализация построенной сети: весов полученных ячеек сети и расстояний между ячейками;     \n* Кластеризация полученных ячеек методом R-Means;    \n* Визуализация результатов кластеризации;    \n* Вычисление метрик близости и однородности кластеров:\n  - silhouette = 0.46\n  - homogeneity = 0.23\n  - completeness = 0.55   \n  Метрики могут незначительно меняться, так как зафиксировать воспроизводимость результатов при применении simpSOM невозможно.    \n* Расчет и визуализация распределения значений признаков для разных кластеров.","metadata":{}},{"cell_type":"markdown","source":"**Шаги кластеризации с размерности при помощи автокодировщика:**    \n* Построение нейронной сети типа Autoencoder. Для кодировщика были взяты 3 плотных слоя с нарастанием количества нейронов + добавлен скрытый слой с малым количеством нейронов (\"бутылочное горлышко\"). Затем для декодирования были взяты 3 плотных слоя с уменьшением количества нейронов.    \n* Обучение автокодировщика с применением callback для ранней остановки при выходе на плато метрики loss.     \n* Получение предсказания кодировщиком на пространстве признаков с выходом в виде скрытого слоя (\"бутылочное горлышко\" пониженной размерности).    \n* Кластеризация предсказания пониженной размерности методами ML с указанием количества кластеров:    \n  - K-Means (silhouette=0.48, homogeneity=0.10, completeness=1.00),    \n  - Agglomerative Clustering (silhouette=0.47, homogeneity=0.10, completeness=1.00),     \n  - Birch (silhouette=0.45, homogeneity=0.05, completeness=1.00),   \n  - Gaussian Mixture (silhouette=0.41, homogeneity=0.11, completeness=1.00).  \n* Разведывательная кластеризация методами, автоматически определяющими количество кластеров:    \n  - DBSCAN,   \n  - OPTICS.    \n  Данные методы показали крайне неудовлетворительный результат по сегментации и метрикам, что свидетельствует о плохо структурированном пространстве признаков.     \n* Ансамблирование по голосованию для 3 методов, показавших схожие результаты (K-Means, Agglomerative Clustering, Gaussian Mixture) с предварительным переименованием кластеров. Отнесение кластера к единому шаблону выполнялось на основании близости центроидов. Метрики ансамбля:       \n  - silhouette = 0.48\n  - homogeneity = 0.10\n  - completeness = 1.00    \n  Кластеризация методом BIRCH выдало разбиение, существенно отличающееся от трех других. В связи с этим данная сегментация рассматривалась отдельно и не включалась в ансамбль.","metadata":{}},{"cell_type":"markdown","source":"**Выдача результатов сегментирования заключалась в следующем:**   \n* Для каждого кластера из каждого способа разбиения (K-Means, SOM, ансамбль по автокодировщику, BIRCH по автокодировщику) были рассчитаны статистики и в зависимости от них указаны признаки:   \n  - Кластер с самым большим количеством судебных дел,     \n  - Кластер с самой большой суммой долга,    \n  - Кластер с самой маленькой суммой оплаты,   \n  - Кластер с самым большим количеством контактной информации.    \n* Указания, какие методы работы с должником применять в зависимости от его признаков, были даны в файле с шаблоном определения методов. Данный шаблон разрабатывался отделом работы с должниками. Шаблон был наложен на рассчитанные признаки по каждому должнику и по нему определена рекомендация.      \n* Рекомендации с обоснованием их выдачи были сохранены в итоговый файл.     \n* Рекомендация по каждому должнику была дана для каждого спсоба сегментрирования, то есть в 4-х вариантах. Выбор варианта оставлен за пользователем, то есть, за сотрудником отдела работы с должниками.   ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}