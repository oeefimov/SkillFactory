{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для подсчета количества страниц на первой странице поиска объявлений о модели\n",
    "def brand_pages(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    pages_cnt = page.find_all(class_ = \"Button Button_color_whiteHoverBlue Button_size_s Button_type_link Button_width_default ListingPagination-module__page\")\n",
    "    \n",
    "    if pages_cnt != []:\n",
    "        last_tag = pages_cnt[-1]\n",
    "        cnt_text = last_tag.find(class_ = \"Button__text\").text\n",
    "        return int(cnt_text)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для извлечения всех ссылок на автомобили на текущей странице очередной модели\n",
    "def get_auto_links(url):\n",
    "    \n",
    "    response = requests.get(url, headers={'User-Agent': UserAgent().chrome})\n",
    "    response.encoding ='utf8'\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    links = []\n",
    "    hrefs = page.find_all('a', class_='Link ListingItemTitle-module__link')\n",
    "    \n",
    "    for href in hrefs:\n",
    "        links.append(href.get(\"href\"))\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция по извлечению данных со страницы автомобиля в словарь auto\n",
    "def get_auto_data(url):\n",
    "    \n",
    "    response = requests.get(url, headers={'User-Agent': UserAgent().chrome})\n",
    "    response.encoding ='utf8'\n",
    "    page = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Будем заполнять сллварь, так как его потом легко превратить в датафрейм\n",
    "    auto = {}  \n",
    "    auto['car_url'] = url\n",
    "    auto['parsing_unixtime'] = int(time.time())\n",
    "    \n",
    "    try:\n",
    "        for script in page.find_all(\"script\"):\n",
    "            try:\n",
    "                if 'complectation\":{\"id\"' in str(script):\n",
    "                    scr = str(script)  \n",
    "                    auto['complectation_dict'] = re.search(r'complectation\":{\"id.*?}', scr)[0][15:]\n",
    "            except:\n",
    "                auto['complectation'] = None\n",
    "\n",
    "            try:\n",
    "                if 'equipment\":{' in str(script):\n",
    "                    scr = str(script) \n",
    "                    auto['equipment_dict'] = re.search(r'equipment\":{.*?}', scr)[0][11:]\n",
    "            except:\n",
    "                auto['equipment_dict'] = None\n",
    "\n",
    "            try:\n",
    "                if '\"model_info\":' in str(script):\n",
    "                    scr = str(script)  \n",
    "                    auto['model_info'] = re.search(r'\"model_info\":{.*?}', scr)[0][13:]\n",
    "                    auto['model_name'] = re.search(r'model_info\":{\"code\":\".*?\"', scr)[0][20:].strip('\"')\n",
    "            except:\n",
    "                auto['model_info'] = None\n",
    "                auto['model_name'] = None        \n",
    "            try:\n",
    "                if 'super_gen\":{' in str(script):\n",
    "                    scr = str(script)  \n",
    "                    auto['super_gen'] = re.search(r'super_gen\":{.*?}', scr)[0][11:]\n",
    "            except:\n",
    "                auto['equipment_dict'] = None        \n",
    "            try:       \n",
    "                if 'vendor\":\"' in str(script):\n",
    "                    scr = str(script)  \n",
    "                    auto['vendor'] = re.search(r'vendor\":\".*?\"', scr)[0][9:].strip('\"')\n",
    "            except:\n",
    "                auto['vendor'] = None        \n",
    "\n",
    "        for tag in page.find_all('div'):\n",
    "            if tag.get(\"title\") == \"Идентификатор объявления\":\n",
    "                auto['sell_id'] = re.search(r'\\d+', tag.text)[0]\n",
    "\n",
    "\n",
    "        Breadcrumbs = page.find_all('a', class_=\"Link Link_color_gray CardBreadcrumbs__itemText\")\n",
    "        auto['brand'] = Breadcrumbs[0].text\n",
    "        auto['model_name'] = Breadcrumbs[1].text\n",
    "        auto['bodyType'] = Breadcrumbs[3].text\n",
    "\n",
    "\n",
    "        tags = page.find_all('li', class_=\"CardInfoRow CardInfoRow_year\")\n",
    "        auto['productionDate'] = tags[0].text[-4:]\n",
    "\n",
    "        tags = page.find_all('li', class_=\"CardInfoRow CardInfoRow_color\")\n",
    "        auto['color'] = tags[0].text[4:]\n",
    "\n",
    "        tags = page.find_all('li', class_=\"CardInfoRow CardInfoRow_engine\")\n",
    "        txt = tags[0].text.replace(' ','').replace('\\xa0л.с.','').split('/')\n",
    "        auto['engineDisplacement'] = txt[0][9:12]\n",
    "        auto['enginePower'] = txt[1]\n",
    "        auto['fuelType'] = txt[2]\n",
    "\n",
    "        tags = page.find_all('li', class_=\"CardInfoRow CardInfoRow_transmission\")\n",
    "        txt = str(tags)\n",
    "        st = txt.rpartition('<span class=\"CardInfoRow__cell\">')[2].partition('</span></li>]')\n",
    "        auto['vehicleTransmission'] = st[0]\n",
    "\n",
    "        tags = page.find_all('li', class_=\"CardInfoRow CardInfoRow_kmAge\")\n",
    "        txt = str(tags)\n",
    "        st = txt.rpartition('<span class=\"CardInfoRow__cell\">')[2].partition('</span></li>]')\n",
    "        auto['mileage'] = st[0].replace('\\xa0','').replace('км','')\n",
    "\n",
    "        try:\n",
    "            tags = page.find_all('div', class_=\"CardDescription__textInner\")\n",
    "            txt = str(tags)\n",
    "            st = txt.partition('<span>')[2].partition('</span>')\n",
    "            auto['description'] = st[0].replace('<br/>',' ')\n",
    "        except:\n",
    "            auto['description'] = None \n",
    "\n",
    "\n",
    "        tags = page.find_all('span', {'class': 'OfferPriceCaption__price'})\n",
    "        st = tags[0].text.replace('\\xa0','')\n",
    "        auto['priceCurrency'] = st[-1]\n",
    "        auto['price'] = st[:-1]\n",
    "\n",
    "        tags = page.find_all('a', {'class': 'Link Link_color_black'})\n",
    "        auto['productionDate'] = tags[0].text\n",
    "\n",
    "        # Часть информации ушла со страницы объявления о продаже автомобиля\n",
    "        # За пропавшей информацией перейдем на страницу модели\n",
    "        tags = page.find_all('a', {'class': 'Link SpoilerLink CardCatalogLink SpoilerLink_type_default'})\n",
    "        link_model = tags[0].get(\"href\")\n",
    "\n",
    "        tags = page.find_all('a', {'class': 'Link SpoilerLink CardCatalogLink SpoilerLink_type_default'})\n",
    "        link = tags[0].get(\"href\")\n",
    "        responseM = requests.get(link, headers={'User-Agent': UserAgent().chrome})\n",
    "        responseM.encoding ='utf8'\n",
    "        pageM = BeautifulSoup(responseM.text, 'html.parser')\n",
    "\n",
    "        tagsM = pageM.find_all('div', {'class': 'search-form-v2-mmm search-accordion i-bem'})\n",
    "        strM = tagsM[0].get(\"data-bem\")\n",
    "        j = json.loads(strM)\n",
    "        ge = j['search-form-v2-mmm']['selectedMmmNames']['generation']\n",
    "        auto['modelDate'] = ge.replace(' ','').split('–')[0][-4:]\n",
    "\n",
    "        # Вместо измененного super_gen, в котором теперь лежит совсем другая информация,\n",
    "        # отдельно вытащим данные, которые некогда лежали в этом поле\n",
    "        tagsM = pageM.find_all('dt', {'class': 'list-values__label'})\n",
    "        i = -1\n",
    "        acc = -1\n",
    "        nd = -1\n",
    "        cl =-1\n",
    "        fr =-1\n",
    "        for tagM in tagsM:\n",
    "            i += 1\n",
    "            if tagM.text=='Разгон до 100 км/ч, с':\n",
    "                acc = i\n",
    "            if tagM.text=='Количество дверей':\n",
    "                nd = i\n",
    "            if tagM.text=='Клиренс':\n",
    "                cl = i\n",
    "            if tagM.text=='Расход топлива, л город/трасса/смешанный':\n",
    "                fr = i\n",
    "        \n",
    "        tagsM = pageM.find_all('dd', {'class': 'list-values__value'})\n",
    "        auto['numberOfDoors'] = tagsM[nd].text\n",
    "        \n",
    "        try:\n",
    "            auto['acceleration'] = tagsM[acc].text\n",
    "        except:\n",
    "            auto['acceleration'] = None\n",
    "        try:    \n",
    "            auto['clearance_min'] = tagsM[cl].text\n",
    "        except:\n",
    "            auto['clearance_min'] = None\n",
    "        try:\n",
    "            auto['fuel_rate'] = tagsM[fr].text\n",
    "        except:\n",
    "            auto['fuel_rate'] = None\n",
    "        \n",
    "\n",
    "        span_CardInfoRow__cell = page.find_all('span', {'class': 'CardInfoRow__cell'})\n",
    "        for i,tag in enumerate (span_CardInfoRow__cell):\n",
    "            try:\n",
    "                if tag.text == \"Владельцы\":\n",
    "                    auto['Владельцы'] = span_CardInfoRow__cell[i+1].text.replace(u'\\xa0', u' ') \n",
    "            except:\n",
    "                auto['Владельцы'] = None\n",
    "            try:        \n",
    "                if tag.text == \"Владение\":\n",
    "                    auto['Владение'] = span_CardInfoRow__cell[i+1].text.replace(u'\\xa0', u' ') \n",
    "            except:\n",
    "                auto['Владение'] = None\n",
    "            try:\n",
    "                if tag.text == \"ПТС\":\n",
    "                    auto['ПТС'] = span_CardInfoRow__cell[i+1].text.replace(u'\\xa0', u' ') \n",
    "            except:\n",
    "                auto['ПТС'] = None\n",
    "\n",
    "            if tag.text == \"Привод\":\n",
    "                auto['Привод'] = span_CardInfoRow__cell[i+1].text.replace(u'\\xa0', u' ') \n",
    "            if tag.text == \"Руль\":\n",
    "                auto['Руль'] = span_CardInfoRow__cell[i+1].text.replace(u'\\xa0', u' ') \n",
    "\n",
    "            try:\n",
    "                if tag.text == \"Состояние\":\n",
    "                    auto['Состояние'] = span_CardInfoRow__cell[i+1].text.replace(u'\\xa0', u' ')\n",
    "            except:\n",
    "                auto['Состояние'] = None\n",
    "            try:\n",
    "                if tag.text == \"Таможня\":\n",
    "                    auto['Таможня'] = span_CardInfoRow__cell[i+1].text.replace(u'\\xa0', u' ') \n",
    "            except:\n",
    "                auto['Таможня'] = None\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачивались только те марки, которые были в test и в тренировочном наборе, данном по умолчанию.\n",
    "Русские марки не скачивались, так как они сильно отличаются от иномарок.\n",
    "Редкие марки не скачивались, тоже чтобы не нарушать статистическую однородность.\n",
    "Автомобили скачивались только до 2020 года выпуска. Год 2021 не брался, так как это новые автомобили."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приводится список использованных марок, но скачивание производилось не для всех сразу, а порциями в несколько дней. \n",
    "Для этого список разбивался на части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PONTIAC: 1 стр.\n",
      "1\n",
      "MINI: 6 стр.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Список марок автомобилей\n",
    "# brands = ['SKODA', 'AUDI', 'HONDA', 'VOLVO', 'BMW', 'NISSAN', 'INFINITI',\n",
    "#            'MERCEDES', 'TOYOTA', 'LEXUS', 'VOLKSWAGEN', 'MITSUBISHI',\n",
    "#            'CADILLAC', 'CHERY', 'CHEVROLET', 'CHRYSLER',\n",
    "#            'CITROEN', 'DAEWOO', 'DODGE', 'FORD', 'GEELY', 'HYUNDAI',\n",
    "#            'JAGUAR', 'JEEP', 'KIA', 'MAZDA', 'MINI',\n",
    "#            'OPEL', 'PEUGEOT', 'PORSCHE', 'RENAULT',\n",
    "#            'SUBARU', 'SUZUKI', 'GREAT_WALL', 'LAND_ROVER', 'SSANG_YONG']\n",
    "\n",
    "brands = ['PONTIAC','MINI']  \n",
    "\n",
    "df = pd.DataFrame()       # датафрейм для закачки данных одной марки автомобиля\n",
    "\n",
    "for brand in brands:\n",
    "    url_brand_list = []   # список ссылок на отдельные страницы данных одной марки автомобиля\n",
    "    \n",
    "    url = 'https://auto.ru/moskva/cars/' + brand.lower() + '/used/?year_to=2020'\n",
    "    max_page = brand_pages(url) + 1\n",
    "        \n",
    "    for page in range(1, max_page):\n",
    "        url = url + '&page=' + str(page)\n",
    "        url_brand_list.append(url)\n",
    "\n",
    "    # Получаем список списков ссылок на объявления о всех автомобилях текущей марки со всех страниц\n",
    "    page_list = [] \n",
    "    try:\n",
    "        page_list = Parallel(n_jobs = 2)(delayed(get_auto_links)(url) for url in url_brand_list)\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "    # Информируем, сколько страниц данной марки надо скачивать\n",
    "    print(brand + ': ' + str(len(page_list)) + ' стр.')\n",
    "    i = 0\n",
    "    \n",
    "    auto_dict_list = [] # список словарей с данными по автомобилям на текущей странице\n",
    "\n",
    "    # Перебираем отдельные страницы для текущей марки автомобиля\n",
    "    for links in page_list:\n",
    "        i += 1\n",
    "        \n",
    "        # По каждой ссылке на текущей странице парсим данные и распараллеливаем этот процесс\n",
    "        try:\n",
    "            auto_dict_list = Parallel(n_jobs = 2)(delayed(get_auto_data)(url) for url in links)\n",
    "        except Exception as e:\n",
    "            print('Ошибка: ' + str(e))\n",
    "            pass\n",
    "        \n",
    "        # Информируем, какая по счету страница скачалась\n",
    "        print(i)\n",
    "\n",
    "        # В датасет добавляем очередной автомобиль\n",
    "        for auto in auto_dict_list:\n",
    "            df = df.append(auto, ignore_index=True)   \n",
    "            \n",
    "    # Сохраняем данные по текущей марке автомобиля\n",
    "    df.to_csv('carMsk-' + brand + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собранные датасеты по отдельным маркам впоследствии были слиты в один набор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким же образом были собраны данные и по другим городам, но в обучении модели наличие других городов только ухудшило метрику. \n",
    "Поэтому реально использовались данные только из Москвы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
